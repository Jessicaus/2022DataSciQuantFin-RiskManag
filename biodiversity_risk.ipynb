{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_HF_yNHkVah"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S2IYC14cSsS"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from rasterstats import zonal_stats\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "i5FY_6wgkXVu",
        "outputId": "8d3f76d3-db89-4f44-e724-74ea9fde736a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ortools\n",
            "  Downloading ortools-9.15.6755-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting absl-py>=2.0.0 (from ortools)\n",
            "  Downloading absl_py-2.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: numpy>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from ortools) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ortools) (2.2.2)\n",
            "Collecting protobuf<6.34,>=6.33.1 (from ortools)\n",
            "  Downloading protobuf-6.33.5-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from ortools) (4.15.0)\n",
            "Requirement already satisfied: immutabledict>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from ortools) (4.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->ortools) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->ortools) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->ortools) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->ortools) (1.17.0)\n",
            "Downloading ortools-9.15.6755-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (29.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.8/29.8 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.4.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.33.5-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, absl-py, ortools\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.6\n",
            "    Uninstalling protobuf-5.29.6:\n",
            "      Successfully uninstalled protobuf-5.29.6\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.5 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed absl-py-2.4.0 ortools-9.15.6755 protobuf-6.33.5\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "f010ed5df4d24df1962d7e380bba744e",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install ortools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD4DQtoIklJc"
      },
      "outputs": [],
      "source": [
        "from ortools.init.python import init\n",
        "from ortools.linear_solver import pywraplp\n",
        "from ortools.sat.python import cp_model\n",
        "import pulp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmOak5MFFv3m"
      },
      "source": [
        "change matplotlib graph font"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFN7j-IIH_RG",
        "outputId": "99ae589e-4d22-4c93-c83a-e567facc12a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Phlya/adjustText.git\n",
            "  Cloning https://github.com/Phlya/adjustText.git to /tmp/pip-req-build-fdxj0dhl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Phlya/adjustText.git /tmp/pip-req-build-fdxj0dhl\n",
            "  Resolved https://github.com/Phlya/adjustText.git to commit aab1e1915f0b995b0b0e53811f6aeedfabbe618a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from adjustText==1.3.0) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from adjustText==1.3.0) (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from adjustText==1.3.0) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText==1.3.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText==1.3.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText==1.3.0) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText==1.3.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText==1.3.0) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText==1.3.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText==1.3.0) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->adjustText==1.3.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->adjustText==1.3.0) (1.17.0)\n",
            "Building wheels for collected packages: adjustText\n",
            "  Building wheel for adjustText (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adjustText: filename=adjustText-1.3.0-py3-none-any.whl size=13146 sha256=1c5760bc070c5a70e334da84e33fc9c89f2c578c3603480ce087c8d911669982\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1nkvtbhq/wheels/3f/87/33/6793fa00952ed8b44482264b36599f69b330f0151c4b03d6ef\n",
            "Successfully built adjustText\n",
            "Installing collected packages: adjustText\n",
            "Successfully installed adjustText-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/Phlya/adjustText.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcTI92IGJ4Pf"
      },
      "outputs": [],
      "source": [
        "from adjustText import adjust_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yngpSyMfFzNM",
        "outputId": "b8c60ac5-7302-48ce-a3d3-67f301cb7b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2026-02-12 00:00:50--  https://github.com/google/fonts/raw/main/ofl/cardo/Cardo-Regular.ttf\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/google/fonts/main/ofl/cardo/Cardo-Regular.ttf [following]\n",
            "--2026-02-12 00:00:50--  https://raw.githubusercontent.com/google/fonts/main/ofl/cardo/Cardo-Regular.ttf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400468 (391K) [application/octet-stream]\n",
            "Saving to: ‘Cardo-Regular.ttf’\n",
            "\n",
            "Cardo-Regular.ttf   100%[===================>] 391.08K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2026-02-12 00:00:50 (64.8 MB/s) - ‘Cardo-Regular.ttf’ saved [400468/400468]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O Cardo-Regular.ttf https://github.com/google/fonts/raw/main/ofl/cardo/Cardo-Regular.ttf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9IVfY4IF4ul"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS33-hsVF6a5"
      },
      "outputs": [],
      "source": [
        "font_path = 'Cardo-Regular.ttf'\n",
        "\n",
        "# Add the font to the Matplotlib font manager\n",
        "fm.fontManager.addfont(font_path)\n",
        "\n",
        "# Extract the font name (this should be 'Cardo')\n",
        "prop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "r3nSR-FthnNi",
        "outputId": "520535dc-f58e-4195-bd7b-5e0158db1070"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il-g0pKLkRWk"
      },
      "source": [
        "## Biodiversity Risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hMMkDsdhu90"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/Colab Notebook/MCM 2026/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCZ5gjImcWAa"
      },
      "outputs": [],
      "source": [
        "sites = {'Index': range(0, 10),\n",
        "         'Names': ['Pacific Spaceport Complex (Alaska)', 'Vandenberg Space Force Base (California)', 'Boca Chica / Starbase (Texas)', 'Kennedy Space Center (Florida)', 'Wallops Flight Facility (Virginia)', 'Baikonur Cosmodrome (Kazakhstan)', 'Guiana Space Centre (French Guiana)', 'Satish Dhawan Space Centre (India)', 'Taiyuan Satellite Launch Center (China)', 'Mahia Peninsula (New Zealand)'],\n",
        "         'Latitudes': [57.435330, 34.772040, 25.996000, 28.608200, 37.846210, 45.955150, 5.237390, 13.737400, 38.849086, -39.260440],\n",
        "         'Longitudes': [-152.33931, -120.60124, -97.154, -80.6040, -75.47938, 63.35028, -52.76950, 80.23510, 111.608497, 177.86431]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG59H-fsrU2Q"
      },
      "outputs": [],
      "source": [
        "elevator = {'Index': range(0, 3),\n",
        "            'Names': ['West of Padang, Indonesia', 'Galapagos Highlands',  'Marsabit Desert'],\n",
        "            'Latitudes': [-0.94924,-0.6333, 2.7044],\n",
        "            'Longitudes': [100.35427,-90.3667, 37.26305]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ_VwYORhVvN"
      },
      "outputs": [],
      "source": [
        "sites_gpd = gpd.GeoDataFrame(sites, geometry=gpd.points_from_xy(sites['Longitudes'], sites['Latitudes']), crs=\"EPSG:4326\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf_JxoJHrVlK"
      },
      "outputs": [],
      "source": [
        "elevator_gpd = gpd.GeoDataFrame(\n",
        "    elevator,\n",
        "    geometry=gpd.points_from_xy(elevator['Longitudes'], elevator['Latitudes']),\n",
        "    crs=\"EPSG:4326\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7-baG55jrZv4"
      },
      "outputs": [],
      "source": [
        "sites_gpd = pd.concat([sites_gpd, elevator_gpd], ignore_index=True)\n",
        "sites_gpd['Index'] = sites_gpd.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SjhW1G4Ci-JT"
      },
      "outputs": [],
      "source": [
        "# Projected to an equidistant projection\n",
        "sites_buf = sites_gpd.to_crs(\"ESRI:54032\").buffer(45000).to_crs(epsg=4326)\n",
        "sites_gpd['geometry'] = sites_buf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ctQa3Xy_iXPh"
      },
      "outputs": [],
      "source": [
        "shp_files = [\n",
        "    \"Protected Areas/WDPA_WDOECM_Jan2026_Public_all_shp_0/WDPA_WDOECM_Jan2026_Public_all_shp-polygons.shp\",\n",
        "    \"Protected Areas/WDPA_WDOECM_Jan2026_Public_all_shp_1/WDPA_WDOECM_Jan2026_Public_all_shp-polygons.shp\",\n",
        "    \"Protected Areas/WDPA_WDOECM_Jan2026_Public_all_shp_2/WDPA_WDOECM_Jan2026_Public_all_shp-polygons.shp\"\n",
        "]\n",
        "#pa_gdf = [gpd.read_file(path + f) for f in shp_files]\n",
        "#pa_combined = pd.concat(pa_gdf, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukbtGTTx4USc"
      },
      "outputs": [],
      "source": [
        "# total_bounds = sites_gpd.to_crs(\"EPSG:4326\").total_bounds # [minx, miny, maxx, maxy]\n",
        "total_bounds = tuple(sites_gpd.to_crs(\"EPSG:4326\").total_bounds)\n",
        "\n",
        "pa_gdf = []\n",
        "for f in shp_files:\n",
        "    # This only loads features that intersect the bounding box of ALL your sites\n",
        "    data = gpd.read_file(path + f, bbox=total_bounds, columns = ['geometry'])\n",
        "    pa_gdf.append(data)\n",
        "\n",
        "pa_combined = pd.concat(pa_gdf, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnUpJPxKuePc"
      },
      "outputs": [],
      "source": [
        "sites_mollweide = sites_gpd.to_crs(\"ESRI:54009\") # the iUCN data is mollweide projection, also equispace\n",
        "pa_mollweide = pa_combined.to_crs(\"ESRI:54009\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-N_oaF8kPjf"
      },
      "outputs": [],
      "source": [
        "# intersections = gpd.overlay(sites_mollweide, pa_mollweide, how=\"intersection\")\n",
        "# intersections['Area'] = intersections.geometry.area / 10**6 # square meters to square kms\n",
        "# area_per_site = intersections.groupby('Names')['Area'].sum().reset_index()\n",
        "pa_candidates = gpd.sjoin(pa_mollweide, sites_mollweide[['Names', 'geometry']], how=\"inner\", predicate=\"intersects\")\n",
        "\n",
        "# 3. Now run the overlay ONLY on those candidates\n",
        "# This preserves the 'Names' column from the sites_mollweide side\n",
        "intersections = gpd.overlay(sites_mollweide[['Names', 'geometry']], pa_candidates[['geometry']], how=\"intersection\")\n",
        "\n",
        "# 4. Calculate Area and Group\n",
        "intersections['Area'] = intersections.geometry.area / 10**6\n",
        "area_per_site = intersections.groupby('Names')['Area'].sum()\n",
        "\n",
        "# 5. Merge back to your main GeoDataFrame\n",
        "sites_gpd = sites_gpd.merge(area_per_site, on='Names', how='left')\n",
        "sites_gpd['Area'] = sites_gpd['Area'].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCvZyv4bp2wD"
      },
      "outputs": [],
      "source": [
        "area_per_site = sites_gpd[['Names']].merge(area_per_site, on='Names', how='left')\n",
        "area_per_site['Area'] = area_per_site['Area'].fillna(0)\n",
        "sites_gpd['Area'] = area_per_site['Area']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQl5eAVchluX"
      },
      "outputs": [],
      "source": [
        "# Species Richness\n",
        "stats = zonal_stats(sites_mollweide, path + \"species_richness.tif\", stats=\"mean\", all_touched = True)\n",
        "sites_gpd['Mean Richness'] = [s['mean'] for s in stats]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjdwliQU0WD1"
      },
      "outputs": [],
      "source": [
        "sites_gpd['Biodiversity Risk'] = np.log1p(sites_gpd['Area'] + 1) * sites_gpd['Mean Richness']\n",
        "max_risk = sites_gpd['Biodiversity Risk'].max()\n",
        "sites_gpd['Biodiversity Risk'] = sites_gpd['Biodiversity Risk'] / max_risk\n",
        "sites_gpd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlAnGIAXAczI"
      },
      "outputs": [],
      "source": [
        "rocket_name_map = {\n",
        "    \"Alaska\": \"Pacific Spaceport Complex (Alaska)\",\n",
        "    \"California\": \"Vandenberg Space Force Base (California)\",\n",
        "    \"Texas\": \"Boca Chica / Starbase (Texas)\",\n",
        "    \"Florida\": \"Kennedy Space Center (Florida)\",\n",
        "    \"Virginia\": \"Wallops Flight Facility (Virginia)\",\n",
        "    \"Kazakhstan\": \"Baikonur Cosmodrome (Kazakhstan)\",\n",
        "    \"France\": \"Guiana Space Centre (French Guiana)\",\n",
        "    \"India\": \"Satish Dhawan Space Centre (India)\",\n",
        "    \"China\": \"Taiyuan Satellite Launch Center (China)\",\n",
        "    \"New Zealand\": \"Mahia Peninsula (New Zealand)\",\n",
        "}\n",
        "\n",
        "elev_name_map = {\n",
        "    \"Padang\": \"West of Padang, Indonesia\",\n",
        "    \"Galapagos\": \"Galapagos Highlands\",\n",
        "    \"Marsabit\": \"Marsabit Desert\",\n",
        "}\n",
        "\n",
        "risk_lookup = sites_gpd.set_index(\"Names\")[\"Biodiversity Risk\"]\n",
        "\n",
        "sites = list(rocket_name_map.keys())\n",
        "sites_eco = {k: float(risk_lookup[full]) for k, full in rocket_name_map.items()}\n",
        "elev_eco  = {k: float(risk_lookup[full]) for k, full in elev_name_map.items()}\n",
        "\n",
        "print(sites_eco)\n",
        "print(elev_eco)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHLTpqnDPzQi"
      },
      "source": [
        "## Ozone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R-HrAP21dMX"
      },
      "outputs": [],
      "source": [
        "rocket_emissions = pd.read_csv(path + \"rocket_emissions_with_sources.csv\")\n",
        "rocket_emissions['liftoff_t'] = rocket_emissions[\"liftoff_mass_kg\"] /1000\n",
        "rocket_emissions['payload_t'] = rocket_emissions[\"payload_leo_kg\"] /1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF_1X7N319aV"
      },
      "outputs": [],
      "source": [
        "def run_analysis(df, x_col, y_col, x_label, y_label):\n",
        "    x, y = df[x_col], df[y_col]\n",
        "    m, b = np.polyfit(x, y, 1)\n",
        "    r_squared = np.corrcoef(x, y)[0,1]**2\n",
        "\n",
        "    plt.rcParams['font.family'] = prop.get_name()\n",
        "    plt.rcParams['figure.dpi'] = 120\n",
        "\n",
        "    plt.rcParams['mathtext.fontset'] = 'custom'\n",
        "    plt.rcParams['mathtext.rm'] = prop.get_name()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(x, y, color='#525151', label='Data Points')\n",
        "    plt.plot(x, m*x + b, color='#4071CF', label=rf'Fit: y={m:.3f}x + {b:.3f}')\n",
        "\n",
        "    texts = []\n",
        "    for i, txt in enumerate(df['rocket']):\n",
        "        # plt.annotate(txt, (x[i], y[i]), fontsize=8, alpha=0.7)\n",
        "        texts.append(plt.text(x[i], y[i], txt, fontsize=14))\n",
        "\n",
        "    adjust_text(texts,\n",
        "                only_move={'points':'y', 'texts':'y'}, # Prioritize moving vertically\n",
        "                arrowprops=dict(arrowstyle='->', color='gray', lw=0.5))\n",
        "\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.title(f'$R^2 = {r_squared:.4f}$')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.show()\n",
        "    return m, b, r_squared\n",
        "\n",
        "fits = [\n",
        "    ('liftoff_t', 'payload_t', 'Liftoff Mass (t)', 'Payload to LEO (t)'),\n",
        "    ('payload_t', 'bc_t_per_launch', 'Payload to LEO (t)', 'BC Emissions (t)'),\n",
        "    ('liftoff_t', 'bc_t_per_launch', 'Liftoff Mass (t)', 'BC Emissions (t)')\n",
        "]\n",
        "# m, b, r2 = run_analysis(rocket_emissions, 'liftoff_t', 'payload_t', 'Liftoff Mass (t)', 'Payload to LEO(t)')\n",
        "for x, y, xl, yl in fits:\n",
        "    m, b, r2 = run_analysis(rocket_emissions, x, y, xl, yl)\n",
        "    print(f\"Fit for {yl}: slope={m:.4f}, intercept={b:.4f}, R2={r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuJlotSph-oV"
      },
      "outputs": [],
      "source": [
        "def fit_power_law(E, du):\n",
        "    # Fits |du| = a * E^b\n",
        "    x = np.log(E)\n",
        "    y = np.log(-du)  # Use absolute magnitude\n",
        "    b, loga = np.polyfit(x, y, 1)\n",
        "    return np.exp(loga), b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBlkyaOgnLue"
      },
      "outputs": [],
      "source": [
        "# from Maloney's simulation: E (Gg/yr), dO3 (DU), sigma (DU)\n",
        "E_DATA     = np.array([10.0, 30.0, 100.0])\n",
        "DU_DATA    = np.array([-3.5, -9.05, -20.0])\n",
        "SIGMA_DATA = np.array([1.3, 1.0, 1.2]) # confidence +- -1.3, -1.0, -1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52sW0KhGh2Es"
      },
      "outputs": [],
      "source": [
        "# monte carlo, confidence\n",
        "def get_mc_bands(n=10000, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    E_plot = np.logspace(0.5, 2.5, 100) #10^0.5 to 10^2.5, generate 100\n",
        "    x_log = np.log(E_DATA)\n",
        "    samples = []\n",
        "\n",
        "    for _ in range(n):\n",
        "        while True:\n",
        "            du_s = rng.normal(DU_DATA, SIGMA_DATA)\n",
        "            if np.all(du_s < 0): break\n",
        "\n",
        "        # Fit sampled data\n",
        "        b_s, log_a_s = np.polyfit(x_log, np.log(-du_s), 1) # x = log(E_data), y = log(-du_s)\n",
        "        samples.append(-np.exp(log_a_s) * np.power(E_plot, b_s))\n",
        "\n",
        "    return E_plot, np.array(samples) #x, and fitted values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR1pcNRmh8IT"
      },
      "outputs": [],
      "source": [
        "a_fit, b_fit = fit_power_law(E_DATA, DU_DATA)\n",
        "E_plot, mc_samples = get_mc_bands()\n",
        "lo, hi = np.percentile(mc_samples, [2.5, 97.5], axis=0) #lo[i] = 2.5th percentile DU at E_plot[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wcsbBjnh6uz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# confidence band by monte carlo\n",
        "plt.fill_between(E_plot, lo, hi, color='skyblue', alpha=0.3, label='95% MC Confidence Band')\n",
        "\n",
        "plt.plot(E_plot, -a_fit * np.power(E_plot, b_fit), color='#F23F3F', lw=2,\n",
        "         label=rf'Fit: $\\Delta O_3 = -{a_fit:.3f} \\cdot E^{{{b_fit:.3f}}}$')\n",
        "\n",
        "# Original Data Points\n",
        "plt.errorbar(E_DATA, DU_DATA, yerr=SIGMA_DATA, fmt='ko', capsize=5, label='Maloney et al. Data')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Black Carbon Emissions (E) [Gg/yr]')\n",
        "plt.ylabel('Ozone Anomaly ($\\Delta O_3$) [DU]')\n",
        "# plt.title('Ozone Depletion')\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, which=\"both\", alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(rf\"Refined Power Law: dO3 = -{a_fit:.4f} * E^{b_fit:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eic1gLXvxyxI"
      },
      "outputs": [],
      "source": [
        "def calculate_ozone_reduction(payload_total):\n",
        "    bc_tonnes = 0.199564 * payload_total + 0.389833\n",
        "    bc_gg = bc_tonnes / 1000.0\n",
        "    a = 0.640596\n",
        "    b = 0.755395\n",
        "\n",
        "    delta_du = -a * np.power(bc_gg, b)\n",
        "\n",
        "    return delta_du, bc_tonnes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38mEtZClkELG"
      },
      "outputs": [],
      "source": [
        "reduction, bc_mass = calculate_ozone_reduction(180000)\n",
        "print(f\"Total BC: {bc_mass:.2f} tonnes ({bc_mass/1000:.3f} Gg)\")\n",
        "print(f\"Predicted Ozone Change: {reduction:.4f} DU\")\n",
        "print(f\"Impact relative to Maloney 30Gg case: {reduction / -9.05:.4%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjuXGPG_DQKP"
      },
      "source": [
        "### Solve Biodiversity Mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCG__Wa1Dkxs"
      },
      "outputs": [],
      "source": [
        "boost = pd.read_csv(path+'/launch-site-velocity.csv')\n",
        "sites = ['Alaska', 'California', 'Texas', 'Florida', 'Virginia', 'Kazakhstan', 'France', 'India', 'China', 'New Zealand']\n",
        "boost['Sites'] = sites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sltjXvxqDzMq"
      },
      "outputs": [],
      "source": [
        "def cpd_site(site, tons):\n",
        "  return 5.3 * 10**5 * boost.loc[boost['Sites'] == site, 'Velocity Boost'].iloc[0]\n",
        "# C_deltaV, 1 = $5.3 × 105/(M t · km/s)\n",
        "print(cpd_site('France', 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkfo3BXTFGwX"
      },
      "outputs": [],
      "source": [
        "def get_bc_from_payload(payload_t):\n",
        "    return 0.199564 * payload_t + 0.389833\n",
        "\n",
        "def get_max_allowable_bc(limit_du):\n",
        "    a, b = 0.640596, 0.755395\n",
        "    # E = (|ΔO3| / a)^(1/b)\n",
        "    max_bc_gg = (abs(limit_du) / a)**(1/b)\n",
        "    return max_bc_gg * 1000  # Convert Gg to Tonnes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5EoEqVl_CJ0"
      },
      "outputs": [],
      "source": [
        "def solve_eco_balanced_budget( # ozone as a constraint, biodiversity minimized\n",
        "    target_payload=55928,\n",
        "    monthly_budget=100_000_000_000,\n",
        "    rocket_cap=100,\n",
        "    climber_cap=49,\n",
        "    site_eco = None,\n",
        "    elev_eco = None,\n",
        "    # rocket_cost=7950,\n",
        "    climber_cost=2101880, #C)E = $2.10188M/Mt\n",
        "    climbers_per_day_limit=30, # based on the total 179,000\n",
        "    ozone_limit_du = 15,\n",
        "    eco_scale = 100\n",
        "):\n",
        "    model = cp_model.CpModel()\n",
        "    days = 31\n",
        "    sites = ['Alaska', 'California', 'Texas', 'Florida', 'Virginia', 'Kazakhstan', 'France', 'India', 'China', 'New Zealand']\n",
        "    elevators = ['Padang', 'Galapagos', 'Marsabit']\n",
        "\n",
        "    max_bc = get_max_allowable_bc(ozone_limit_du)\n",
        "    bc_per_launch = get_bc_from_payload(rocket_cap)\n",
        "    bc_limit = int(max_bc * 1000) # in kg, not tons\n",
        "    bc_per = int(bc_per_launch * 1000) # kg per launch\n",
        "\n",
        "    site_launch_costs = {s: int(cpd_site(s, rocket_cap)) for s in sites}\n",
        "    climber_event_cost = int(climber_cap * climber_cost)\n",
        "\n",
        "    R = {(s, d): model.NewBoolVar(f'R_{s}_{d}') for s in sites for d in range(1, days+1)}    # Climber is now an Integer (Multi-launch per day)\n",
        "    C = {(e, d): model.NewIntVar(0, 15, f'C_{e}_{d}')\n",
        "         for e in elevators for d in range(1, days + 1)}\n",
        "\n",
        "    # constraints\n",
        "    total_r = sum(R.values())\n",
        "    total_c = sum(C.values())\n",
        "\n",
        "    # Ozone\n",
        "    model.Add(total_r * bc_per <= bc_limit)\n",
        "\n",
        "    # A. Mass\n",
        "    model.Add(total_r * rocket_cap + total_c * climber_cap >= int(target_payload))\n",
        "\n",
        "    # B. Budget\n",
        "    rocket_costs = sum(R[s, d] * site_launch_costs[s] for s in sites for d in range(1, days+1))\n",
        "    model.Add(rocket_costs + (total_c * climber_event_cost) <= monthly_budget)\n",
        "\n",
        "    # C. Rocket Turnaround (2 days: launch on day 1, cannot launch on day 2)\n",
        "    for s in sites:\n",
        "        for d in range(1, days):\n",
        "            model.Add(R[s, d] + R[s, d+1] <= 1)\n",
        "\n",
        "    # climb limit\n",
        "    for d in range(1, days + 1):\n",
        "        model.Add(sum(C[e, d] for e in elevators) <= climbers_per_day_limit)\n",
        "\n",
        "   # MINIMIZE biodiversity risk\n",
        "    # minimize the sum of (launch * site_risk); scaled by 100 to handle float risks in integer solver\n",
        "    # total_bio_risk = sum(R[s, d] * int(site_eco[s] * 100) for s in sites for d in range(1, days+1))\n",
        "    rocket_bio = sum(R[s, d] * int(site_eco[s] * eco_scale)\n",
        "                     for s in sites for d in range(1, days + 1))\n",
        "    elev_bio = sum(C[e, d] * int(elev_eco[e] * eco_scale)\n",
        "                   for e in elevators for d in range(1, days + 1))\n",
        "    total_bio_risk = rocket_bio + elev_bio\n",
        "    model.Minimize(total_bio_risk)\n",
        "\n",
        "    # solve\n",
        "    solver = cp_model.CpSolver()\n",
        "    solver.parameters.max_time_in_seconds = 30.0\n",
        "    status = solver.Solve(model)\n",
        "\n",
        "    if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:\n",
        "        actual_total_cost = sum(solver.Value(R[s, d]) * site_launch_costs[s] for s in sites for d in range(1, days+1))\n",
        "        actual_total_cost += solver.Value(total_c) * climber_event_cost\n",
        "\n",
        "        actual_r_launches = solver.Value(total_r)\n",
        "        actual_bc_t = actual_r_launches * bc_per_launch\n",
        "\n",
        "        # Calculate final Ozone Change for the report\n",
        "        a, b = 0.640596, 0.755395\n",
        "        final_du = -a * ((actual_bc_t / 1000)**b)\n",
        "\n",
        "        # print(f\"--- SUCCESS: {target_payload} TONS DELIVERED ---\")\n",
        "        # print(f\"Total Biodiversity Risk: {solver.Value(total_bio_risk)/100:.2f}\")\n",
        "        # print(f\"Final Ozone Anomaly: {final_du:.4f} DU (Limit: {ozone_limit_du})\")\n",
        "        # print(f\"Rockets: {solver.Value(total_r)} launches\")\n",
        "        # print(f\"Climbers: {solver.Value(total_c)} climbs\")\n",
        "        # print(f\"Peak Daily Operations: {solver.Value(max_daily_events)}\")\n",
        "        # print(f\"Total Monthly Cost: ${ actual_total_cost:,.2f}\")\n",
        "\n",
        "        # print(\"\\n\" + \"=\"*50)\n",
        "        # print(f\"{'DAY':<5} | {'ROCKET SITES ACTIVE':<35} | {'CLIMBER EVENTS'}\")\n",
        "        # print(\"-\" * 50)\n",
        "\n",
        "        \"\"\"for d in range(1, days + 1):\n",
        "            # Find which sites are launching today\n",
        "            active_sites = [s for s in sites if solver.Value(R[s, d]) == 1]\n",
        "            sites_str = \", \".join(active_sites) if active_sites else \"--\"\n",
        "\n",
        "            # Sum up climber activities across all tethers for the day\n",
        "            daily_climbers = sum(solver.Value(C[t, d]) for t in range(num_tethers))\n",
        "\n",
        "            print(f\"{d:<5} | {sites_str:<35} | {daily_climbers} climbers\")\n",
        "\n",
        "        print(\"=\"*50)\"\"\"\n",
        "        return {\n",
        "            \"payload\": target_payload,\n",
        "            \"bio_risk\": solver.Value(total_bio_risk) / 100.0,\n",
        "            \"ozone_anomaly\": final_du,\n",
        "            \"rockets\": actual_r_launches,\n",
        "            \"climbers\": solver.Value(total_c),\n",
        "            \"cost\": actual_total_cost\n",
        "        }\n",
        "    else:\n",
        "        print(\"Infeasible: The physical capacity of your sites/tethers is still too low.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWaf0HDTGkbr"
      },
      "outputs": [],
      "source": [
        "# sites_eco = dict(zip(sites, sites_gpd['Biodiversity Risk']))\n",
        "# test\n",
        "solve_eco_balanced_budget(target_payload = 38000,ozone_limit_du = -1, site_eco=sites_eco, elev_eco = elev_eco)\n",
        "# $62076014672  * 12 * 219 years = $1.63 *10^14 = 163 trillion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18Zyqbx9t5dG"
      },
      "outputs": [],
      "source": [
        "# no rockets, max payload\n",
        "solve_eco_balanced_budget(target_payload = 38000,rocket_cap = 0, ozone_limit_du = -1, site_eco=sites_eco, elev_eco = elev_eco)\n",
        "# $79921885120  * 12 * 219 years = $2.10 *10^14 = 210 trillion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc9J_MUWvhgU"
      },
      "outputs": [],
      "source": [
        "# only rockets\n",
        "solve_eco_balanced_budget(target_payload = 38000,climbers_per_day_limit = 0, ozone_limit_du = -1, site_eco=sites_eco, elev_eco = elev_eco)\n",
        "solve_eco_balanced_budget(target_payload = 8800,climbers_per_day_limit = 0, ozone_limit_du = -1, site_eco=sites_eco, elev_eco = elev_eco)\n",
        "# $692711152 * 12 * 946 years = 7.87 trillion\n",
        "# this is way lower than the \"estimate\"d cost from part 1(a)\n",
        "# which we think is because that didn't take into account launch site difference;\n",
        "# but when the plan is spaced out into 946 years it can only launch at those places"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El9Ngj24QiUt"
      },
      "outputs": [],
      "source": [
        "test_payloads = np.linspace(10000, 60000, 15)\n",
        "results = []\n",
        "\n",
        "print(f\"{'Payload (Tons)':<15} | {'Rockets':<10} | {'Bio Risk':<10}\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "for p in test_payloads:\n",
        "    data = solve_eco_balanced_budget(target_payload=p, ozone_limit_du = -1, site_eco=sites_eco, elev_eco=elev_eco)\n",
        "    if data:\n",
        "        results.append(data)\n",
        "        print(f\"{int(data['payload']):<15} | {data['rockets']:<10} | {data['bio_risk']:<10.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UKYX3haqpqr"
      },
      "outputs": [],
      "source": [
        "x_payload = [r['payload'] for r in results]\n",
        "y_risk = [r['bio_risk'] for r in results]\n",
        "y_rockets = [r['rockets'] for r in results]\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(8, 7))\n",
        "\n",
        "# axis 1: Biodiversity Risk\n",
        "ax1.plot(x_payload, y_risk, marker='s', linestyle='-', color='forestgreen', linewidth=2, label='Min. Bio Risk')\n",
        "ax1.set_xlabel('Payload Capacity (t)', fontsize=12)\n",
        "ax1.set_ylabel('Total Biodiversity Risk Score', fontsize=12)\n",
        "ax1.tick_params(axis='y')\n",
        "ax1.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# axis 2: Rocket Count\n",
        "ax2 = ax1.twinx()\n",
        "ax2.step(x_payload, y_rockets, where='post', color='grey', linestyle='--', alpha=0.5, label='Rocket Count')\n",
        "ax2.set_ylabel('Rocket Launches', color='grey', fontsize=12)\n",
        "ax2.tick_params(axis='y', labelcolor='grey')\n",
        "\n",
        "#plt.title('Minimizing Biodiversity Risk vs. Increasing Payload', fontsize=14, fontweight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI8b1_EjQjTE"
      },
      "source": [
        "### Solve for Ozone Optimized (constraints on biodiversity risk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GYz7js5B1Ol"
      },
      "outputs": [],
      "source": [
        "def solve_ozone_optimized_budget(\n",
        "    target_payload=55928,\n",
        "    monthly_budget=100_000_000_000,\n",
        "    rocket_cap=100,\n",
        "    climber_cap=49,\n",
        "    site_eco=None,\n",
        "    elev_eco=None,\n",
        "    climber_cost=2101880,\n",
        "    climbers_per_day_limit=30,\n",
        "    bio_risk_limit = 210.0, # maximum cumulative biorisk score\n",
        "    eco_scale=100,\n",
        "):\n",
        "    model = cp_model.CpModel()\n",
        "    days = 31\n",
        "    sites = ['Alaska', 'California', 'Texas', 'Florida', 'Virginia', 'Kazakhstan', 'France', 'India', 'China', 'New Zealand']\n",
        "    elevators = ['Padang','Galapagos','Marsabit']\n",
        "\n",
        "    bc_per_launch = get_bc_from_payload(rocket_cap)\n",
        "    site_launch_costs = {s: int(cpd_site(s, rocket_cap)) for s in sites}\n",
        "    climber_event_cost = int(climber_cap * climber_cost)\n",
        "\n",
        "    # Variables\n",
        "    R = {(s, d): model.NewBoolVar(f'R_{s}_{d}') for s in sites for d in range(1, days+1)} # site s launches on day d\n",
        "    C = {(e, d): model.NewIntVar(0, 15, f'C_{e}_{d}') # how many climber trips happen on elevator e on day d. 15 is arbitrary\n",
        "         for e in elevators for d in range(1, days+1)}\n",
        "\n",
        "    total_r = sum(R.values())\n",
        "    total_c = sum(C.values())\n",
        "\n",
        "    # constraints\n",
        "\n",
        "    # B. Mass\n",
        "    model.Add(total_r * rocket_cap + total_c * climber_cap >= int(target_payload))\n",
        "\n",
        "    # C. Budget Constraint\n",
        "    rocket_costs = sum(R[s, d] * site_launch_costs[s] for s in sites for d in range(1, days+1))\n",
        "    model.Add(rocket_costs + (total_c * climber_event_cost) <= monthly_budget)\n",
        "\n",
        "    # D. Rocket Turnaround (2 days)\n",
        "    for s in sites:\n",
        "        for d in range(1, days):\n",
        "            model.Add(R[s, d] + R[s, d+1] <= 1)\n",
        "\n",
        "    # cap limit for elevators\n",
        "    for d in range(1, days+1):\n",
        "        model.Add(sum(C[e, d] for e in elevators) <= climbers_per_day_limit)\n",
        "\n",
        "    rocket_bio = sum(R[s, d] * int(site_eco[s] * eco_scale) for s in sites for d in range(1, days+1))\n",
        "    elev_bio   = sum(C[e, d] * int(elev_eco[e] * eco_scale) for e in elevators for d in range(1, days+1))\n",
        "    total_bio  = rocket_bio + elev_bio\n",
        "\n",
        "    # biodiversity must be below constraint\n",
        "    model.Add(total_bio <= int(bio_risk_limit * eco_scale))\n",
        "\n",
        "    # OBJECTIVE: minimize ozone impact\n",
        "    # This is achieved by minimizing the total number of rocket launches (total_r)\n",
        "    # because more rockets = more BC = more ozone depletion.\n",
        "    model.Minimize(total_r)\n",
        "\n",
        "    # solve\n",
        "    solver = cp_model.CpSolver()\n",
        "    solver.parameters.max_time_in_seconds = 30.0\n",
        "    status = solver.Solve(model)\n",
        "\n",
        "    if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:\n",
        "        actual_r = solver.Value(total_r)\n",
        "        actual_bc_t = actual_r * bc_per_launch\n",
        "        a, b = 0.640596, 0.755395\n",
        "        final_du = -a * ((actual_bc_t / 1000)**b)\n",
        "\n",
        "        actual_total_cost = sum(solver.Value(R[s, d]) * site_launch_costs[s] for s in sites for d in range(1, days+1))\n",
        "        actual_total_cost += solver.Value(total_c) * climber_event_cost\n",
        "        elev_totals = {e: sum(solver.Value(C[e, d]) for d in range(1, days+1)) for e in elevators}\n",
        "\n",
        "        # print(f\"--- SUCCESS: {target_payload} TONS DELIVERED ---\")\n",
        "        # print(f\"Objective: Minimize Ozone Depletion\")\n",
        "        # print(f\"Final Ozone Anomaly: {final_du:.4f} DU\")\n",
        "        # print(f\"Total Biodiversity Risk: {solver.Value(total_risk_val)/100:.2f} (Limit: {bio_risk_limit})\")\n",
        "        # print(f\"Rockets: {actual_r} launches\")\n",
        "        # print(f\"Climbers: {solver.Value(total_c)} climbs\")\n",
        "        # print(f\"Total Monthly Cost: ${actual_total_cost:,.2f}\")\n",
        "        return {\n",
        "            \"payload\": target_payload,\n",
        "            \"ozone_anomaly\": final_du,\n",
        "            \"total_bc_tons\": actual_bc_t,\n",
        "            \"rockets\": actual_r,\n",
        "            \"climbers\": solver.Value(total_c),\n",
        "            \"elevator_climbs\": elev_totals,\n",
        "            \"bio_risk\": solver.Value(total_bio) / eco_scale,\n",
        "            \"cost\": actual_total_cost\n",
        "        }\n",
        "    else:\n",
        "        print(\"Infeasible: The biodiversity limit or budget is too tight for the required payload.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98gWxgJ5HsIi"
      },
      "outputs": [],
      "source": [
        "# sites_eco = dict(zip(sites, sites_gpd['Biodiversity Risk']))\n",
        "solve_ozone_optimized_budget(target_payload = 16500, bio_risk_limit=50,site_eco=sites_eco, elev_eco = elev_eco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtyYOr8rM8ak"
      },
      "outputs": [],
      "source": [
        "test_payloads = np.concatenate((np.linspace(10000, 45000, 5),np.linspace(45000, 60000,15)))\n",
        "results = []\n",
        "\n",
        "print(f\"{'Payload (Tons)':<15} | {'Rockets':<10} | {'Ozone Anomaly (DU)':<20}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Run the loop and print results\n",
        "for p in test_payloads:\n",
        "    data = solve_ozone_optimized_budget(target_payload=p, site_eco=sites_eco, elev_eco = elev_eco)\n",
        "    if data:\n",
        "        results.append(data)\n",
        "        print(f\"{int(data['payload']):<15} | {data['rockets']:<10} | {data['ozone_anomaly']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijlv8w9rr6PJ"
      },
      "outputs": [],
      "source": [
        "x_payload = [r['payload'] for r in results]\n",
        "y_ozone = [r['ozone_anomaly'] for r in results]\n",
        "y_rockets = [r['rockets'] for r in results]\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(8, 7))\n",
        "\n",
        "# 1st: ozone\n",
        "color_ozone='#3F68D1'\n",
        "ax1.set_xlabel('Payload Capacity (t)', fontsize=12)\n",
        "ax1.set_ylabel('Ozone Anomaly (DU)', fontsize=12)\n",
        "ax1.plot(x_payload, y_ozone, marker='o', linestyle='-', label='Ozone Depletion', linewidth=2)\n",
        "ax1.tick_params(axis='y')\n",
        "ax1.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# 2nd: rocket launches\n",
        "ax2 = ax1.twinx()\n",
        "color_rockets = 'tab:grey'\n",
        "ax2.set_ylabel('Number of Rocket Launches', color='grey', fontsize=12)\n",
        "# We use step for rockets because they are discrete whole numbers\n",
        "ax2.step(x_payload, y_rockets, where='post', color='grey', linestyle='--', label='Rocket Count', alpha=0.7)\n",
        "ax2.tick_params(axis='y', labelcolor='grey')\n",
        "\n",
        "#plt.title('Minimizing Ozone vs. Increasing Payload', fontsize=14, fontweight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hObCtEr6JHzD"
      },
      "outputs": [],
      "source": [
        "payload_range = np.linspace(0, 100000, 500)\n",
        "ozone_impacts = calculate_ozone_reduction(payload_range)[0]\n",
        "\n",
        "success_payload = 45 * 100 # test\n",
        "success_ozone = -0.5993\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(payload_range, ozone_impacts, label='Ozone Depletion Curve', color='blue', lw=2)\n",
        "\n",
        "plt.scatter(success_payload, success_ozone, color='red', zorder=5,\n",
        "            label=f'Success Point ({success_payload}t Rocket Payload)')\n",
        "\n",
        "plt.axhline(0, color='black', linewidth=0.8)\n",
        "plt.title('Stratospheric Ozone Decrease vs. Total Rocket Payload', fontsize=14)\n",
        "plt.xlabel('Total Rocket-Delivered Payload (Tonnes)', fontsize=12)\n",
        "plt.ylabel('Global Mean Ozone Anomaly ($\\Delta O_3$) [DU]', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "\n",
        "plt.annotate(f' {success_ozone} DU', (success_payload, success_ozone),\n",
        "             textcoords=\"offset points\", xytext=(0,10), ha='center', fontweight='bold')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}